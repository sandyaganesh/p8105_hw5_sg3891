---
title: "p8105_hw5_sg3891"
author: Sandya Ganesh
date: 2021-11-20
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

Read in and clean the data.

```{r clean}
raw_homicide_df =
  read_csv("./data/homicide-data.csv", na = c("", "Unknown"), show_col_types = FALSE)

homicide_df = raw_homicide_df %>% 
  mutate(
    city_state = str_c(city, state),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
      )) %>% 
  relocate(city_state) %>% 
  filter(city_state != "TulsaAL")
```


The total number of observations in the homicide data set from the Washington Post is `r nrow(raw_homicide_df)` observations with `r ncol(raw_homicide_df)` variables. Some key variables in this data set include `r names(raw_homicide_df %>% select(c(8, 9, 12)))`. 

In the code chunk below we summarize within cities to obtain the total number of homicides and the number of unsolved homicides, and run a prop test for a single city (Baltimore).

```{r summary}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "BaltimoreMD")

baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test =
prop.test(
  x = baltimore_summary %>% pull(unsolved),
  n = baltimore_summary %>% pull(n))

baltimore_test %>% 
  broom::tidy()
```

In the 2 code chunks below we create a function to run prop.test for each of the cities in the dataset, and extract both the proportion of unsolved homicides and the confidence interval for each.

```{r}
prop_test_function = function(city_df) {
  
  city_summary =
    city_df %>%
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n()
    )

  city_test =
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n))
  
  return(city_test)
}

```

```{r}
results_df = 
  homicide_df %>% 
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))

```

Finally, we plot the data to show estimates and confidence intervals for each city

```{r city_ci_plot}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Proportion estimates and CIs for unsolved homicides",
    x = "Cities",
    y = "Proportion of Unsolved Homicides ",
    caption = "Data from Washington Post")
```

## Problem 2

The code chunks for problem 2 utilize a longitudinal study data set with an experimental and control arm. 


#### Clean and tidy data

In the code chunk below we create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time

```{r, message = FALSE}
long_data = 
  tibble(
  files = list.files("./data/zip_data/")) %>%
  mutate(
    subject_data = map(files, ~read_csv(file.path("./data/zip_data/",.), show_col_types = FALSE))
  ) %>%
  unnest(subject_data) %>%
  mutate(
    arm = ifelse(str_detect(files, "con"), "Control", "Experimental"),
    subject_id = substr(files,5L,6L)
  ) %>% 
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  select(arm, subject_id, week, observation)
  
```

#### Spaghetti Plot
In the code chunk below we make a spaghetti plot that displays observations on each subject over time, and comment on differences between groups.

```{r spaghetti_plot}
long_data %>% 
  group_by(subject_id, week) %>% 
  ggplot(aes(x = week, y = observation, color = subject_id)) + 
  geom_line(aes(group = subject_id)) + 
  geom_point() +
  facet_grid(. ~arm) +
  labs(
    title = "Observations for Experimental and Control Arms in Longitudinal Study",
    x = "Week",
    y = "Observation",
    color = "subject_id",
    caption = "Data from the longitudinal set")
```

Looking at the control group, there is no obvious trend in observations across the 8 weeks, with values tending to remain the same on average across the study. However in the experimental group, we can see that there is a clear upwards trend, with the observation values increasing as weeks increase. Therefore, comparing the two arms, we can see that while at week 1 the observation values were fairly similar between the two arms, at week 8 (by the end of study), the observation values for the experimental group are much higher than the control group. 

## Problem 3

#### Introduce missing values into iris dataset
```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

#### Write and apply function to replace missing values
* For numeric variables, missing values are replaced by the mean of non-missing values
* For character variables, missing values are replaced with "virginica"

```{r}
fill_missing = function(vector) {

  if (is.numeric(vector)) {
    vector = replace_na(vector, round(mean(vector, na.rm = TRUE), 2))
  }
  
  if (is.character(vector)) {
    vector = replace_na(vector, "virginica")
  }
  
  if (!is.character(vector) & !is.numeric(vector)) {
  vector = "Warning: Neither numeric nor character"
  }
  
  return(vector)

}

iris_without_missing = map_df(iris_with_missing, fill_missing)

iris_without_missing
```
